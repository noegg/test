**Part 2: Trainers and Training-related Interventions **

**Lead Researcher: Carol Waters**

This is a trainer-focused study that endeavours to gain a deeper and broader sense of what digital security trainers do, how their work has evolved, and what they have observed working with HRDs over 15+ years of digital security trainings. The second study found in Part 3, 'Security in Context', looks at the experiences of human rights defenders (HRDs) during trainings and their experiences adopting digital security and privacy practices.

**Research context**

This research should be understood against the backdrop of recent trends focused on improving existing approaches to digital security training. These intiatives emerged for several reasons, including the increased profile of the sector, leading to an increase in the number digital security trainings held,and—according to the trainers interviewed—an increasing number of new, inexperienced trainers and organizations leading some of them. These trends prompted recent initiatives that are attempting to improve the dominant models of and approaches to digital security training, as well initiatives to convene previously decentralised and disparate training communities to collaborate, coordinate, and professionalize their work. Organisation-specific training initiatives include TTC's Holistic Security Programme, IREX's S.A.F.E. Programme for journalists at risk, Frontline Defenders' regional Digital Security Consultants, and the two Digital Integrity Fellowship programs led by Hivos and the Open Technology Fund. Efforts to bring the wider digital security training community together for collaboration, coordination, and professional development include the LevelUp Program at Internews (which TTC has partnered with since its inception), and trainer-specific gatherings around the development and dissemination of TTC's Holistic Security curriculum. It is also important to add that despite these important interventions, there has been little to no research evaluating the practices and processes of digital security trainings.

**Factors outside the scope of this research:** It is crucial to note that many of the pivotal factors—frequently unknown or unclear—that affect the aspirations of digital security training-related activities for HRDs are not the objects of analysis here. Trainers often gain a sense of what *some* of the factors and dynamics are that effective adoption of safer digital practices Examples of these factors include anything outside of the scope of what trainers observe or are involved in that may enable or inhibit uptake of improved digital and holistic security practices by both individuals and groups of HRDs. These independent variables represent other opportune--yet challenging--areas for research. Some of the findings from the trainer interviews in this study touch on these crucial uptake factors, but they were not the focus of this study because they often remain largely opaque to trainers.

**Research questions: **

The inspiration for the overall project were questions Tactical Tech was asking about our work in digital security capacity building, including:

-   How do we know that what human rights activists learn in our digital security trainings help them change their behaviour and adopt new and safer digital practices?

-   What is the ‘after-life’ of trainings?

-   How do HRDs adapt to changing digital threats?

This trainer-focused portion of the research initially attempted to analyse what uniquely characterised "outstanding" trainers, but this required divorcing trainers from their context in the training room, an inversion of contextual research focused on HRD's learning experiences the 'Security in Context' approach in Part I. This ended up being discarded, since trainers' work is just as unique and contextual vis-a-vis each unique training experience as HRD training participants' experiences. This not only made it impossible to control all the changing variables of a training to analyse trainers, but it excluded the unique expertise and adaptability that distinctly *characterises* effective trainers, for whom no two trainings are the same.

**Research approach and methodology**

This qualitative research consisted of semi-structured interviews with trainers that took place over seven months in 2014. Interviewees were selected using snowball sampling, with an initial list based on both TTC’s network as well the primary researcher’s network based on her work with the broader Freedom of Expression community and recent intensive work launching LevelUp[4], an initiative supporting the global digital security training community. Approximately 60 trainers were contacted with a request for an interview, approximately half of these trainers responded, and 23 individuals were then interviewed. 15 interviews were conducted remotely and eight were conducted in person. Most interviews took place over an hour and half and the researcher took detailed notes. The notes were then coded for analysis. The amount of resulting data for analysis is considerable, and only a fraction of the issues and topics from the trainer interviews is covered here in these findings and analysis. TTC hopes to use the rich collection of remaining interview data for further analysis and documentation to share publicly as well as with the global training community, as well as input for future applied research efforts.

**Role of TTC:** Given the role of TTC in the larger training community as an intermediary, and the deep and complex ties amongst peers we have from our work over the past decade, we felt it was crucial that all the interviews were anonymised. This encouraged interviewees to speak freely. Additionally, since the work trainers do with HRDs is sensitive and can put those they work with at increased levels of risk, great pains were (and will continue to be) taken to ensure that their identities and unique voices aren’t recognisable in any published research. This was in keeping with the ‘do no harm’ ethical framework that informed the participatory research in Part II of this paper. Finally, because of the dearth of research in this area, and in the spirit of the recent rise of collaboration and community building amongst trainers, TTC promised interviewees that we would make our research findings and carefully anonymised interview content available to the broader training community.

**Interviewee profiles:** We sought a breadth of geographic representation and experience, and gender. Every effort was made to ensure a diverse sample representative of the training community worldwide. Because of the snowball method of sampling participants, many had existing connections to TTC. In an attempt to reduce selection bias, we sought interviewees with weak or non-existent ties to TTC and succeeded in interviewing seven of these individuals. Despite these efforts, it is possible that Tactical Tech’s philosophies, methods, and approaches can be seen to have impacted these findings.

**2.1: A Typology of Digital Security Training Events and Approaches**

In order to understand and analyse what trainers had to share about their work, we needed to establish a common language for our analysis and findings, as well as for our conversations with the trainers themselves. Despite the steady increase of digital and physical security training for HRDs over the past 10-15 years, there is still no common set of key definitions for this core set of activities within the Freedom of Expression and HRD Protection communities. Although many individuals use the words ‘training,’ ‘awareness-raising,’ and ‘training-of-trainers,’ experienced trainers know that these are wide intra-sector differences in how trainings are defined and implemented by different actors and organisations. This can lead to confusion and adversely affect the learning experience and expectations of HRDs.

This lack of shared, standardised definitions leads to misconceptions about what various types of training-related events entail, and what outcomes should be expected from them. These misperceptions and resulting misunderstandings emerge in communications with event conveners, organisational leadership, and funders, as will be seen in the findings below

Therefore, the central activity that this research focuses on—a 'training' led by 'trainers' or 'facilitators'— needed as clear and as broadly shared a definition as possible. Defining a training was also needed to define other related types of interventions along the spectrum of direct activities that are designed to help HRDs reduce their digital risks such as awareness-raising, one-on-one support, etc.

This section aims to establish a shared set of baseline definitions for the three most common types of training-related activities and events for readers to understand what each type of training-related activity is called, the scope each activity can have, and how they can be distinguished from one another. This was required to define the focus of the research, and also highlighted a need for the training community to collaboratively establish shared definitions and descriptions of their activities. It is hoped that this effort to arrive at standardised, shared definitions can be used in the future by trainers to more accurately convey their work to those outside of the training community, including colleagues within their own organisations, conveners, HRD organisations, and funders.

**Awareness-raising**

Awareness-raising aims to establish and expand participants' awareness of digital threats, how those threats may affect them, why they should take certain steps to reduce their risk, and may also include some low-level advice on how to do this. It's useful to think of awareness-raising as one type of 'introductory' experience or 'first exposure' as part of a larger spectrum of events and approaches with the same over-arching goal: for HRDs to have the necessary skills, knowledge and ability to use digital tools and services in an informed way to reduce risks according to their unique needs and context. But this end-goal is not met with awareness-raising alone.

Three features of awareness-raising activities help differentiate it from 'traditional' training events. Confusing the two--or using 'training' as a synonym for awareness-raising--leads to a number of negative outcomes, including poorly managed expectations to miscalculated program and event design, all at great costs to HRDs, conveners, and funders. Awareness-raising events can be distinguished from 'traditional' training events (described in the following section) by their scope, length, ratio of participants to leadership, and the skills of who leads these events:

**Scope of issues and content covered:** Awareness-raising advice is generally limited to getting participants to be aware that risks to their privacy and security exist and what those risks are in a broad sense, as well as become introduced to easy steps they can take to improve their security and privacy. Practical advice is primarily focused on changing settings in the tools and services they already use (e.g., making their Facebook accounts private instead of public), choosing certain tools and services over others, and being more aware of the channels of communication they use for certain activities. Often participants are given handouts, guides, or links to further resources to use for self-directed learning.

Awareness-raising events aren't designed to provide in-depth, hands-on learning or tailored assistance on intermediate and advanced approaches and tools required to effectively address the kind of risks typically faced by HRDs. For many HRDs, this is where training, robust self-learning (which most non-technical users are less likely to effectively accomplish alone), or some form of sustained hands-on learning and support is needed, or they risk becoming 'stuck':

> If you have security awareness, you’re mainly just choosing services on the Internet. After that you need to use a tool to take it to a new level. So eventually you hit a wall. An example of this would be email: you’re aware that emails can be read, but your awareness gets stuck and you can’t use any tools that would prevent this. So you end up asking yourself what to do but you get stuck: Do you stop using email? Some of them just stop thinking about it and stop being concerned — others become more interested and learn tools and attend more trainings. (*Digital security trainer with 12 years experience)*

This is how awareness-raising exists on a spectrum of approaches and activities that overlap with training, but they are rarely coordinated this way for HRD audiences. This can leave an at-risk subset of awareness-raising participants in a difficult position where they can either stop caring about privacy and security all together, or find a way to reach the next ‘stage’ of learning and application, which is where training (or its equivalent) comes in.

**Length of time and ratio of participants:** In terms of length, interviewees described awareness-raising events as lasting anywhere from an hour to a day, and permitting a larger number of participants per individual event leader due to the more limited, lecture-based content. In contrast, trainings are 3-5 days and have a trainer-to-participant ratio between 1:3 and 1:12 trainers per participants.

**'Traditional' or 'End-User' Trainings **

Training events are an opportunity for HRDs to gain in-depth knowledge and skills that they can use to improve their safety when using digital tools and services in a way best suited to their context and needs. Many trainings are implemented as 'one-off' or 'stand-alone' trainings, but some are implemented as part of an ongoing learning experience for HRDs.

Core characteristics of trainings[5] include hands-on installation and use of tools, a low participant-to-trainer ratio, and a longer period of event time to sufficiently cover content and enable hands-on practice and tool use. Some trainings may be project- or event-specific and include other types of content (e.g., social media and advocacy training, online publishing, data collection and management, etc.).[6] Others may be holistic security trainings that also integrate physical and psycho-social security. In aggregate, the trainers interviewed provided more detailed properties that define trainings and distinguish them from other types of efforts, particularly awareness-raising events:

**Length of Training Events:** Almost every trainer interviewed said trainings required 3-5 days. In exceptional circumstances, some mentioned that a training could be done over two days at the absolute minimum, but this was considered an inadequate length of time by most. Reasons why a 'proper training' requires 3-5 days included directly related to the following issues:

**Process:** The first day of a training workshop is needed for participants and the trainer(s) to get a sense of each other, establish expectations, and (crucially) for trainers to get a more accurate sense of participants’ skill levels and needs. This in due in part to trainers being unable to get a sufficient sense of what they need to craft a tailored agenda before the workshop, especially if they are unable to conduct a preparatory site visit or have an in-depth ongoing working relationship with the participants.

**Logistics and competing commitments by participants:** Since it is challenging for most participants to get time away from their daily responsibilities, trainings were reported as five days at the longest, but many were 3-4 days, despite participants and trainers wanting more time. Less than five days resulted in less content covered in a more condensed manner, which is not as conducive to learning and retention for participants, who reported returning to their daily lives overloaded with information that they couldn't absorb, process, or implement very well.[7]

**Increased demand for more content to be covered:** As the security and privacy aspects of an ever-increasing number of tools and services need to be addressed, especially in the wake of the Snowden revelations, trainers are being asked to cover more content during trainings. Therefore, as trainings are already straining to cover 'the basics' in a way that doesn't overwhelm participants, they are also being asked to cover additional tools, topics, and fundamental concepts. This adds even more pressure to increase the length of trainings. In response to these issues, trainers have been trying to pilot new training models to employ in addition to the 'traditional' 3-5 day training model. Several trainers described conducting multiple half- or one-day trainings on a regular basis over longer periods of time, and strongly praised this approach for better retention, stronger demonstrable skills and knowledge, and investment from participants.

**Ratio of trainers to participants:** As mentioned further below, interviewees unanimously advocated for having at least more than one trainer for events because it allowed them to provide a better learning experience for participants. The maximum ratio of trainers per participants ranged from 1:3 to 1:12 amongst interviewees.

**Content Covered:** The range of content covered is quite varied and extensive, unless trainers take a 'one-size-fits-all' or limited 'tools-only' approach to content. More broadly, the range of content covered includes mix of hands-on and in-depth background including learning how the Internet works, risk analysis, operating systems, basic computer security (anti-virus, passwords), device security (including mobiles), and more.

**2.2: What enables effective trainings?**

As part of our efforts to gain a sense of what trainers believed led to successful training workshops and long-term impact, we asked our interviewees a series of questions about the ingredients of trainings, which approaches were more effective, and what differentiated novice, experienced, and outstanding trainers.

Before we could consider trainers' observations, methods, and recommendations for what (in their experience) enables effective trainings, we first needed understand what the goals of trainings were—what made a training successful or unsuccessful? When the majority of trainers were asked about what a successful training looks like, their answers were vague, general, or remarkably modest in scope. Success for some of the trainers interviewed included 'participants showing up’, 'helping people’, ‘improving participants' odds and reducing their risks’, and more. Only two of trainers interviewed required explicit learning goals be established for each training that fed directly into their evaluation processes, but this was not a prevailing experience or practice for the majority of trainers interviewed.[8]

**Ingredients for a Successful Training Event**

Based on experience, direct observations, and their personal training approaches and philosophies, interviewees how and why certain elements and approaches are more conducive to an effective training. Given the multitude of unique environments and continually shifting contexts for HRDs, one specific approach 'does not fit all' trainings, and no two trainings are exactly alike. With that in mind, trainers have found that these have led to better training workshops and outcomes for HRD training participants.[9]

**Participants:** Not having appropriate participants was described as a primary cause of bad or failed trainings. Trainers find that trainings are more successful when participants are 'at-risk or working with those at-risk. Having participants who are at-risk (or working with those at-risk) does not necessarily guarantee interest and motivation, however. Secondly, having a similar level of skill level amongst all participants instead of wildly divergent skill levels drastically increases the quality of a training. The exact skill level (from beginner to advanced) doesn't matter, it just needs to be roughly the same for all participants. Otherwise, a great amount of time is spent 'splitting' the teaching to various skill levels in the room, which reduces the overall amount of content that can be covered, as well as the quality of instruction.

**Trainer-to-participant ratios:** On average, interviewees felt that the ratio of trainers to participants for hands-on trainings should be one trainer for every eight participants. A few felt that you could go as high as one trainer for every twelve participants, but it would be at the cost of quality for the participants and drastically limit what a trainer could cover during the event. Ultimately, interviewees described how too many participants per trainer resulted in less effective and inferior learning experiences. In some cases, a poor trainer-to-participant ratio forced trainers to turn nominal trainings into lighter awareness-raising events since they lacked the necessary staff to deliver an in-depth, hands-on event.

**Co-Trainers:** In addition to descriptions of trainer-to-participant ratios, interviewees across the board firmly supported co-training whenever possible. Working with co-trainers resulted in better experiences for participants with trainings that could run smoother and trainers able to offer one-on-one support for participants that a solo trainer couldn’t provide except at the cost of the entire cohort. Co-training let trainers balance each others’ strengths and weaknesses, prevented trainers feeling drained during and after a multi-day training, and enabled trainers to take a rare opportunity to learn from each other. Several trainers talked about how they sought out potential local trainers, but this risked issues during the training if unknown local co-trainers ended up being poorly qualified or unwilling to actually train.

**Agenda:** Matching participants with the content and approach of a training was also an oft-cited essential ingredient. This was often deeply intertwined with a trainer’s skills, experience, and approach. Knowledge of the local and/or participants’ context was mentioned as ideal, but most trainers recognised that they were so often ‘dropped’ into trainings at the last moment that this was not always possible. Even if they were able to prepare properly, trainers also uniformly stated that they wouldn’t ‘actually know’ what the participants’ skills and needs were until the first day of the training, leading to routine adjustments to the agenda on the basis of the first day or half day of the training. Initially developing a well-structured agenda, then observing and adjusting it based on the first day or half-day of a training was described as a hallmark of a skilled trainer. Lastly, trainers prioritised quality of learning over quantity of content covered; they felt covering a small set of content well was better than poorly covering everything at the risk of lower retention by participants. This could also lead to potentially dangerous outcomes if participants took steps based on rushed and poor instruction that led to harmful outcomes, especially with advanced tools and tactics.

**Training environment and creation of a ‘safe space’:** It was not mentioned as frequently, but several trainers described the importance of a safe, comfortable venue. They described the need to create a ‘safe space’ for participants, which is a mix of creating a supportive, non-critical environment as well as a physically safe space. Being able to trust the trainer and fellow participants was a crucial part of this — without it, ‘you might as well not have the training’.

**Preparation:** A majority of the trainers--especially the most experienced interviewees--described adequate preparation as key. Since there are often last-minute requests, trainers described ‘minimal viable prep time’ as a concept they’d deeply internalised. This includes a sufficient amount of time to properly select and assess participants, choose a venue and manage logistics, conduct contextual and technical research, as well as prepare the participants as much as possible. Without this ‘minimally viable prep,’ trainings ran the risk of being poor uses of time and funding. Unfortunately, trainers often have to respond to conveners who have put trainings together at the last minute to ‘check a box’ for grants, or simply due to poor planning and organisation. Trainers prefer to say no to these requests, since 'these trainings just won’t be as successful’, but frequently lead them anyway due to various requirements, pressures, and personal desires to help despite the drawbacks of such events.

**Follow-up:** Despite the fact that follow-up is crucial to learning, retaining, and properly implementing what participants have learned, trainers report that follow-up is almost never funded. Trainers reported great frustration with how follow-up is a necessity for trainings to make an impact but struggled to do it or do it well. Trainers say that true learning begins when the training ends, because this is when participants choose to either start using the tools and knowledge back in their day-to-day lives or decide to not implement what they’ve learned. But despite knowing that this moment is the most delicate and crucial to uptake, trainers are unable to successfully secure the necessary resources to improve outcomes at this decisive stage in the training. Because of this, motivated, at-risk participants tend to return back to their daily lives from workshops and are overwhelmed by their ongoing responsibilities, the challenge of using difficult tools and tactics alone and without organisational or social support, and often with only a fragment of a trainers’ free time that they are able to spare for follow-up. HRD training participants resoundingly supported these same findings in the Security in Context research findings in Part 3.

**Approaches and strategies for effective trainings**

Trainers' descriptions of recommended teaching approaches and methods were one of the richest areas of discussion with trainers; our findings are highly abridged here.

**Understanding and applying Adult Learning Principles (‘Androgogy’):** Although the method and practice of teaching is routinely referred to as ‘pedagogy’, the wider digital (and holistic) security training community recently became more broadly exposed to adult learning theory, also known as ‘androgogy’, to reflect its focus on adults instead of children. Using adult learning methodologies was the most commonly favoured approach by trainers.[10]

**Tailoring trainings to individuals and communities of practice:** The adoption of adult learning principles by trainers has paralleled increased explorations of how trainers can effectively integrate risk assessment (or 'threat modeling') into trainings, which helps participants understand, identify, and prioritize their threats in order to then develop and implement solutions for their specific needs and context. This is at the heart of tailoring digital security trainings to individuals and distinct communities of practice in a participant-driven way, and represents a relatively recent shift away from the lecture-based, top-down, one-size-fits-all, tool-centric model of training which has been a standard for much of the training community for over a decade. When this training model was dominant, there were almost no opportunities for piloting and disseminating new approaches, conducting applied research, or sharing amongst trainers in a collaborative community.

**Rejecting fear and encouraging empowerment**: Several trainers--especially active members of HRD groups and movements--strongly rejected taking a 'fear-based' approach to training, advocating a need for empowerment and confidence-boosting. Others supported this after being introduced to TTC's recent work on Holistic Security. Many of the trainers interviewed described how exposure to adult learning approaches and holistic/integrated security has led to a richer understanding of how to help their participants more effectively.

**Holistic Security**: Parts of the training community are highly supportive and have already adopted Holistic Security approaches co-developed under the lead of TTC. This is in recognition of the false separation between digital and physical threats, and an awareness of how individual well-being can impact the physical and digital safety of HRDs and their networks. TTC has taken a leadership role in this work, collaborating with other intermediary training groups and initiatives. Adapting and implementing Holistic Security was described as a challenging (but worthwhile) progression for trainers, due to the complexity of new domain knowledge. An additional complication was the challenge of integrating additional Holistic Security-related activities and content into already over packed training agendas.

**Using (in)security demonstrations selectively, carefully, and ethically:** Several interviewees described how their positions on 'security' or 'insecurity demos' had changed over time as they moved away from the ‘fear-based, tool-centric’ model of trainings. Live security (or 'insecurity') demos are when trainers demonstrate how an specific type of digital attack is carried out (e.g., capturing passwords as someone logs into an online account, or directly accessing data on a laptop or mobile phone without a password). Trainers have been known to target participants without warning or permission during these demos, which can result in participants' passwords being captured and revealed to a group of participants, or having their laptop accessed while they are out of the room for lunch or break. This can leave participants feeling attacked, embarrassed, or even 'singled out', which can lead to a hostile environment and poor training outcomes. A small number of interviewees even said they would not do demonstrations, but the dominant sentiment was that they are a vital awareness-raising and teaching tool for trainers. One trainer summed up a commonly shared opinion that ‘insecurity demos are THE MOST effective \[teaching\] tactic, but they can easily go astray if you’re not controlling every element carefully.’[11]

**2.3: The current approach to evaluating 'one-off' trainings is broken**

Our interviewees agreed that the current approach to evaluating the effectiveness of 'one-off' digital security trainings—especially *after* a training has ended—is profoundly broken. There are evaluation approaches that trainers feel work well in limited ways *or* as part of sustained, long-term engagements. But one trainer summed up an almost unanimous sentiment of interviewees: ‘I don’t know any good way to do monitoring and evaluation in a meaningful way.’

The reasons for failed evaluation vary, but one of the most common reasons cited by trainers is that most funded trainings are 'one-off' events that do not include support for trainers to conduct necessary follow-up that simultaneously enables post-training evaluation opportunities. This is in contrast with sustained learning engagements that often include multiple trainings, as well as more robust support for HRDs. These not only tend to *have* goals (and an idea of what 'success' should look like), but they also tend to operate at the collective instead of the individual level with wider engagement and support from organizations and networks. All of these characteristics of sustained learning engagements and relationships with participants enable richer opportunities for observation and evaluation in view of certain goals. Aside from communities that already have access to local digital and holistic security expertise, these sustained, long-term learning engagements have been the exception until a number of very recent pilot projects, including Hivos' and OTF's Fellowship program and Frontline Defenders' Digital Security Consultants initiative.

The difficulties of evaluating the impact and 'uptake' of 'one-off' digital and holistic training events are further magnified by unique risks of working with at-risk HRDs: responses to requests for information from participants suffer from low response rates, in part due to the need to communicate securely with at-risk participants. This makes effective evaluation and follow-up more challenging for 'one-off' trainings than for sustained, long-term engagements. Because of this, trainers are often limited to evaluating *during* the event, which does not measure or demonstrate post-training outcomes.

**Evaluation approaches used during training events**

**The end-of-the-training survey is a poor evaluation tool:** In order to ensure a high evaluation response rate, as well as avoid post-training communication challenges, funders, conveners, organisers, and training organisations encourage trainers to conduct end-of-workshop surveys as the preferred vehicle for evaluation. However, trainers resoundingly find these to be unreliable sources of evaluation data and 'a waste of time', despite receiving remarkably high evaluation ratings as trainers via this approach.

More than half of the interviewees described the phenomenon of ‘gratitude bias’ with end-of-training surveys and surveys conducted immediately after the end of a training. One trainer described this well when claiming that the most an end-of-training survey could reflect was

> ...that people were there. When I finish, people want to pay back with what currency they have, and that currency is kindness, so evaluations are hugely biased towards the positive. And I don’t know how to interpret it. Funders love it, but I feel foolish because over 99% are positive — it's like an election turnout in a communist country. *(Trainer and training program manager with more than 10 years experience.)*

Because of this phenomenon, most trainers avoid end-of-training surveys ‘unless an organiser asks for them.’ If end-of-training surveys are conducted, trainers do not consider them to be useful, valid feedback on their work.

**Daily evaluations and trainer debriefs:** Conducting 'plus / delta' assessments from participants at the end of every day is a valued practice. Trainers ask participants to share things they thought were good or particularly great that day ('plusses'), as well as anything they would suggest changing (‘deltas’). These are typically shared via post-its in as anonymous a fashion as possible. Trainers then collect these from participants and have a debrief session with fellow trainers and/or organisers in order to review them. They then discuss how they can adjust the agenda or approach for the following day—or even the entire event—to accommodate requests. At the beginning of the next day, trainers will share a general review of the plusses/deltas from the previous day, first by sharing the things participants liked, then discussing what they will change (or are unable to change) in response to the deltas.

This helps participants be more engaged and helps them co-create their learning experience, as well as feeling heard and respected by trainers. It also results in better trainings, as each day evaluated and calibrated to be more effective. Finally, it provides a very clear source of evaluation during the training, which can be used for post-training evaluation if they're able to gather any post-event evaluation data.

**The limited utility of pre- and post-tests:** Only 3 of 23 trainers interviewed said they conducted ‘pre- and post-tests’ that measured participants’ knowledge and skills. While these pre- and post-test are valuable and should be tried more widely, it is important to remember that they do not evaluate uptake, nor what is being *implemented* and *used* by participants and their peers after an event, only what they can demonstrate they learned *during* the event. One participant[12] <span id="Part_2_Trainers_and_Trainingre" class="anchor"></span>described how much they value pre- and post-event assessments:

> *They’ve helped a ton, more than I thought, because these are actual questions testing their knowledge. They’re mostly general security questions that I ask in different ways before the training, and then in I word the questions differently at the end. Works well and helps you create and adjust an agenda. Some examples of questions are: ‘What is a firewall?’ ‘Do you have antivirus programme? Would having two antivirus programmes be better than one? What is a phishing link?’ \[Without these\], how else would you know how good a trainer would be?*

**The relationship between 'one-off' trainings and fruitless evaluation approaches:** The majority of trainers say the only usable post-training evaluation data they're able to gather is qualitative, often in the form of irregular follow-up interactions with a minority of past participants. This is considered less-than-ideal, but the best option for gathering data with a higher probability of validity. They also often have pre-event assessments and information about the participants’ skills from the training itself for comparison. Despite the shortcomings of this approach overall, one trainer expressed that they would still ‘rather get substantive qualitative’ evaluation data than end-of-training surveys that were glowing, but inaccurate.

One of the major issues with this approach is that trainers consider follow-up crucial to the learning and implementation process, but it is rarely supported or funded for 'one-off' trainings. Several trainers reported spending 10-40% of their time conducing unpaid training follow-up for participants, but it is often at the cost of their well-being and livelihoods, as many trainers can rapidly burn out without proper self-care and reasonable work schedules, even at organisations with core funding for training.

**2.4: Targeting individuals instead of collectives**

The current dominant training model used to help HRDs and their peers often places the greatest amount of responsibility for success on individual HRDs instead of distributing it throughout communities. Not all successful digital capacity-building efforts for HRDs require large amounts of 'top-down' funds and support, but they do need to reach a tipping point where the commitment and motivation to improve the collective and individual safety of the HRD community is widely shared throughout all levels of a collective. This ensures that responsibility is distributed instead of centralised on 1-2 individuals (as 'champions' or in other roles). The findings of this research point to a promising number of ways to explore and pilot new approaches where all members of HRD communities can play a variety of roles in a shared collective effort that is buttressed by sustained long-term learning engagements.

The implicit goal of digital security trainings is to have at-risk HRDs attend workshops in order to understand and improve their digital security. Although these individuals are typically considered to be at-risk because of their activities at a collective level, almost all trainings are designed to target individuals. Nearly all of the trainers interviewed said they rarely or never had workshops with participants from a single organisation or network (but would like to), and were rarely called in to help an organisation as a whole. Presumably, the *hope* is that this individual-focused model eventually scale, either through organisations choosing to adopt and fund organisation-wide improvements, policies, and trainings, or through word-of-mouth and social adoption via human relationships and networks. But the prevailing focus on the individual alone does not reflect the reality of how these communities operate:

> Digital security doesn’t exist in a vacuum — it exists in movements. The value we assign to digital security is to support social movements. The movements are the point — securing and strengthening those movements is the point of what we do. So securing movements is one goal, and strengthening them is another… (Trainer with 16 years of digital security training and teaching experience)

Trainers suspect that not working in a sustained way with committed communities at the network and organisational levels means that many of the trainings focused on individuals fail in the absence of a supportive environment where a security culture can grow and thrive.

The shortcomings of the individual-focused approach has even led to removing one of the most basic stages of learning in the 'one-off' training model. The critical step of providing follow-up support after the training, 'when the real learning begins,' is not funded, appreciated, or considered necessary, which only exacerbates the chronic failure of the approach and model of 'one-off' trainings and puts an unreasonable burden on busy, committed trainers to provide this gratis, which almost all interviewees report being unable to do properly or at all.

**Taking a collective approach to teaching, learning, and implementing secure practice and habits**

Current behavioural research mirrors trainers’ own experience-based findings: Information and human security require collective support, reinforcement, and implementation in order to succeed. Trainers are aware that the heart of this act is at the individual level, with HRDs understanding their risks and being able to make decisions to effectively reduce those risks, which is why trainings require focus on the individuals’ needs and skills. But since security is collective and only as good as the weakest link, the benefits of most current 'one-off' trainings tend to wither at the individual level after a workshop ends without additional follow-up, care, and cultivation. As one trainer put it, ‘what they’ve learned will diminish over time since they don’t have meaningful ways to implement these things.’

Unless a collective of HRDs--as an organisation or network--has made a collective commitment to improve their digital security that entails action, and implementation of a policy, 'one-off' training participants are also having impossible expectations placed on them. They are expected to either implement digital security practices isolated from their peers, or persuade the entire organisation to do so. This is usually complicated by organisations sending participants with the lowest levels of power or decision-making in the organisational structure. In some cases, trainers report organisations that even send participants who no longer work with or for them. Others report organisations that repeatedly send participants to security workshops but make no changes within the organisation.

**The incentives of organisations and networks**

It is interesting to note that whereas digital security trainings and trainers evolved from the circuit riders from the late 1990s and early 2000s, the dynamic and circumstances of digital security workshops and trainers is completely different—and arguably worse—than the model that preceded them. Circuit riders worked with organisations that welcomed their expertise and were committed to making organisation-wide changes to improve the presence and use of digital technologies within their organizations. Circuit riders conducted sustained work with organisations over long periods of time, which included intensive on-site work, ongoing remote support, and regular follow-up visits with organisations they developed deep and long-term relationships with.

Today, the spirit and approach of circuit riding has become inverted for digital security trainings and trainers: They almost never work at the collective level (and target individuals instead), rarely see or experience organisation-wide buy-in and support to improve collective digital safety, and are even treated hostilely or dismissively by organisations. Instead of a deep collaboration with organisations and groups, they are deployed as one-time deliverers of skills and knowledge to individuals at a ‘one-off’ event. Instead of a shared understanding that adoption and proper use of tools and tactics takes time and support, their body of knowledge is expected to be delivered in as short and discrete a period of time as possible, requiring no post-workshop assistance or support for participants to begin to use these tools correctly or well in their daily lives.

**Recommendations **

The following are top-level recommendations based on the findings of this research study.

**Support the development, piloting and iteration of new training models and approaches**

Over the past 3-4 years, the wider training community began to re-evaluate what had become a stale model of 'one-off' trainings, and found it to be lacking. As described above, this ‘old model’ of trainings could be briefly described as ‘one-off, tool-focused, top-down, and “artificially separated” from other security-related realities of HRDs’. This relates not only to the design and execution of training events, but larger overarching issues of strategy and structural gaps and inconsistencies that led to contradictory experiences for HRDs. In aggregate, the findings from trainers and participants in the participatory research findings in Part 2 recommend exploring and piloting new models and approaches that target groups or collectives[13] <span id="Part_2_Trainers_and_Trainingre-1" class="anchor"></span>with sustained learning opportunities over longer periods of time, utilising local resources and expertise, as well as properly supporting follow-up when sustained learning opportunities are *not* possible. Both of these open up new opportunities for evaluation and assessment unavailable in the current model.

There are efforts to pilot new models, including the Digital Security Consultant approach akin to 'circuit riders' from a decade ago, fellowships from the Digital Defenders Partnership and Open Tech Fund to 'embed' trainers at organisations for sustained collective-level learning experiences. It would be invaluable for the outcomes of these pilots to be shared and discussed within the wider training community. They will also be enhanced by TTC's future and ongoing applied research efforts looking at how peer education, awareness-raising, and the in-depth, hands-on learning of trainings can be better coordinated and strategically deployed.

**Co-develop a theory of change for training-related activities**

Currently, there is no clear ‘theory of change’ that articluates how trainings are designed to meet certain goals within a broader process with specific assumptions and preconditions at play. Although the variables, variations, and risks involved in any sensitive work with HRDs naturally leads to uncertainties and safety considerations, interviewees were unable to articulate any discernible over-arching strategic conception of how trainings *worked*. But instead of explaining and defending the vagueness that defines their work, interviewees appeared frustrated with this situation, and discussed ways that certain initiatives were working to change training for the better.

Working with the wider training community to co-develop of a theory of change would be a powerful community-building and collaborative experience for trainers, both internally and externally. They could begin to articulate a coherent set of achievable goals, including assumptions behind these goals, and how certain actions and activities should be considered preconditions for their achievement. This would also be a rich opportunity to collaborate with (and break down the spaces between) parallel efforts such as software development, advocacy, and other sectors that seek to support HRDs.

**Co-develop standards and improve communications and outreach with conveners, funders, and HRD organizations **

Trainers consistently reported that training wasn't well understood, deployed, or used by various actors. In addition to the absence of a considered theory of how training leads to a specific set of changes in HRDs' digital security practices, there was also a reported need to standardize training-related definitions and descriptions. There is a clear need for the training community to better define and describe trainings and training-related activities for the rest of the sector.

**Support emerging efforts towards collaboration and coordination in the training community**

Throughout the interviews, there was enthusiastic support for the emerging environment of collaboration and coordination amongst the wider digital security training community that TTC has played a key role in through initiatives like LevelUp. Trainers described how the quality of their trainings had been increased from exposure to adult learning principles, introductions to how psycho-social considerations impact participants, new methodologies and approaches to event preparation and agenda development, and more.

The simple process of becoming part of a community of practice was perhaps the most valued aspect: many trainers feel isolated and lack connections to fellow trainers; others may only know 1-2 trainers to varying degrees; and many lacked exposure to their fellow trainers’ unique approaches to trainings and professional development. Exposure to fellow trainers and trainings is one of the most valuable means for trainers to learn quickly, or else trainers are only able to learn through trial and error, which comes at a great cost and investment of time. This work has successfully acknowledged trainers and training-related actors as part of a unique, established profession requiring shared standards and professional development.

The trainers interviewed expressed enthusiasm and commitment to existing efforts that seek to bring trainers together and co-develop a shared body of knowledge and professional standards. The existence of such a community also offers up a rich opportunity for co-developing a theory of change as recommended above.

**Focus on the collective **

As discussed at length in the findings in both research studies, the predominant model of ‘one-off’ training events that focus solely on individuals is impaired due to the well-established need for groups to collectively adopt practices and tools to enable any degree of privacy and digital security. Furthermore, people are less likely to adopt tools and practices used during social and collective interactions and activities if their peers choose not to do so, are unable to do so, or are even dismissive or hostile towards doing so. The current strategy for adoption at the social and collective levels—individual HRDs who have participated in digital security trainings—isn't the only approach or the strongest one, especially given the greater range of options and approaches available at the collective level. Approaches and models for how this can be done need to examine the physics of how the current individual-focused model has failed in order to inform to development of new approaches that can be piloted, iterated, and used as another model approach to training.
